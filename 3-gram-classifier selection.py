from sklearn.ensemble import RandomForestClassifier as RF
from sklearn import model_selection
from sklearn.metrics import confusion_matrix
import pandas as pd
import numpy as np
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
import matplotlib.pyplot as plt
import math

avg = lambda items: float(sum(items)) / len(items)
Score = []

subtrainLabel = pd.read_csv('subtrainLabels.csv')
subtrainfeature = pd.read_csv("TFDIF-150feature.csv")
subtrain = pd.merge(subtrainLabel,subtrainfeature,on='Id')
labels = subtrain.Class
subtrain.drop(["Class","Id"], axis=1, inplace=True)
subtrain = subtrain.values

#Random Forest
X_train, X_test, y_train, y_test = model_selection.train_test_split(subtrain,labels,test_size=0.5)
srf = RF(n_estimators=10, n_jobs=-1)
srf.fit(X_train,y_train)
score = srf.score(X_test,y_test)
print(score)
Score.append(score)
y_pred = srf.predict(X_test)
CM1 = confusion_matrix(y_test, y_pred)
y_pred_rf = srf.predict_proba(X_test)[:,1]
#print(y_pred_rf)
fpr1, tpr1, _ = roc_curve(y_test, y_pred_rf,pos_label=2)
roc_auc1 = auc(fpr1,tpr1)
print(roc_auc1)

#GBDT
from sklearn.ensemble import GradientBoostingClassifier
grad_boost = GradientBoostingClassifier(n_estimators=10)
grad_boost.fit(X_train,y_train)
#deviance
print(grad_boost.score(X_test,y_test))
y_pred = grad_boost.predict(X_test)
CM2 = confusion_matrix(y_test, y_pred)
y_pred = grad_boost.predict(X_test)
y_pred_rf = grad_boost.predict_proba(X_test)[:,1]
#print(y_pred_rf)
fpr2, tpr2, _ = roc_curve(y_test, y_pred_rf,pos_label=2)
roc_auc2 = auc(fpr2,tpr2)
print(roc_auc2)

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(X_train,y_train)
clfscore = clf.score(X_test,y_test)
print(clfscore)
y_pred = clf.predict(X_test)
y_pred_rf = clf.predict_proba(X_test)[:,1]
#print(y_pred_rf)
fpr3, tpr3, _ = roc_curve(y_test, y_pred_rf,pos_label=2)
roc_auc3 = auc(fpr3,tpr3)
print(roc_auc3)

#KNN(K-nearest neighborhood)
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train,y_train)
negscore = neigh.score(X_test,y_test)
print(negscore)
y_pred = neigh.predict(X_test)
y_pred_rf = neigh.predict_proba(X_test)[:,1]
#print(y_pred_rf)
fpr4, tpr4, _ = roc_curve(y_test, y_pred_rf,pos_label=2)
roc_auc4 = auc(fpr4,tpr4)
print(roc_auc4)

#XGBoost
from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators=10)
xgb.fit(X_train,y_train)
xgbscore = xgb.score(X_test,y_test)
print(xgbscore)
y_pred = xgb.predict(X_test)
y_pred_rf = xgb.predict_proba(X_test)[:,1]
#print(y_pred_rf)
fpr5, tpr5, _ = roc_curve(y_test, y_pred_rf,pos_label=2)
roc_auc5 = auc(fpr5,tpr5)
print(roc_auc5)

plt.figure(1)
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.plot([0, 1], [0, 1], 'k--')
plt.title("3-gram TFIDF top150 ROC")
plt.plot(fpr1, tpr1,'r',label = 'RF Val AUC = %0.3f' %roc_auc1)
plt.plot(fpr2, tpr2,'b',label = 'GBDT Val AUC = %0.3f' %roc_auc2)
plt.plot(fpr3, tpr3,'y',label = 'GBDT Val AUC = %0.3f' %roc_auc3)
plt.plot(fpr4, tpr4,'g',label = 'GBDT Val AUC = %0.3f' %roc_auc4)
plt.plot(fpr5, tpr5,'brown',label = 'GBDT Val AUC = %0.3f' %roc_auc5)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve (zoomed in at top left)')
plt.legend(loc='best')
plt.show()

plt.figure(2)
plt.xlim(0, 1)
plt.ylim(0.8, 1)
plt.plot([0, 1], [0, 1], 'k--')
plt.title("3-gram TFIDF top150 ROC")
plt.plot(fpr1, tpr1,'r',label = 'RF Val AUC = %0.3f' %roc_auc1)
plt.plot(fpr2, tpr2,'b',label = 'GBDT Val AUC = %0.3f' %roc_auc2)
plt.plot(fpr3, tpr3,'y',label = 'GBDT Val AUC = %0.3f' %roc_auc3)
plt.plot(fpr4, tpr4,'g',label = 'GBDT Val AUC = %0.3f' %roc_auc4)
plt.plot(fpr5, tpr5,'brown',label = 'GBDT Val AUC = %0.3f' %roc_auc5)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve (zoomed in at top left)')
plt.legend(loc='best')
plt.show()